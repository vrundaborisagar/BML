# Experiment 6 and 10 - Random Forest Classifier
## Overview
An ensemble method that builds multiple decision trees on different subsets of data and averages their predictions to improve accuracy and reduce overfitting.

This experiment uses the Wine Quality dataset.

## Dataset Information
* File: winequality-red.csv
* Separator: ; (semicolon)
* Target Variable: quality

## library used
```python
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
```
## code
```python
# Load the dataset
df = pd.read_csv("/content/drive/MyDrive/winequality-red.csv", sep=';')

# Define features (X) and target (y)
X = df.drop("quality", axis=1)
y = df["quality"]

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Initialize the Random Forest Classifier
rf = RandomForestClassifier(n_estimators=100, random_state=42)

# Train the model
rf.fit(X_train, y_train)

# Make predictions on the test set
y_pred = rf.predict(X_test)

# Evaluate the model
print("Accuracy:", accuracy_score(y_test, y_pred))
print("\nClassification Report:\n", classification_report(y_test, y_pred))
print("\nConfusion Matrix:\n", confusion_matrix(y_test, y_pred))

```
## output
```python
Accuracy: 0.659375

Classification Report:
               precision    recall  f1-score   support

           3       0.00      0.00      0.00         1
           4       0.00      0.00      0.00        10
           5       0.72      0.75      0.73       130
           6       0.63      0.69      0.66       132
           7       0.63      0.52      0.57        42
           8       0.00      0.00      0.00         5

    accuracy                           0.66       320
   macro avg       0.33      0.33      0.33       320
weighted avg       0.63      0.66      0.64       320


Confusion Matrix:
 [[ 0  0  1  0  0  0]
 [ 0  0  7  3  0  0]
 [ 0  0 98 31  1  0]
 [ 0  1 31 91  8  1]
 [ 0  0  0 19 22  1]
 [ 0  0  0  1  4  0]]
```

## Conclusion
The Random Forest Classifier achieved an accuracy of 65.93%.

It shows reasonable performance but could be improved with:

* Hyperparameter tuning (e.g., adjusting n_estimators, max_depth).

* Feature scaling or additional preprocessing.

* Balancing the dataset if class imbalance exists.
